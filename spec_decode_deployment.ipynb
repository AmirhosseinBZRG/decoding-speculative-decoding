{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "import deepspeed\n",
    "import torch.distributed as dist\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "dist.init_process_group(backend='nccl')\n",
    "world_size = dist.get_world_size()\n",
    "rank = dist.get_rank()\n",
    "local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "torch.cuda.set_device(local_rank)\n",
    "device = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "print(world_size, rank, local_rank)\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/your/tf/cache/dir\" #Replace with your transformers cache directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_loader(file_path):\n",
    "    \"\"\"\n",
    "    Input: file_path: Path to the json file containing all the queries. \n",
    "        \n",
    "    File format looks like the following:\n",
    "    {\"prompt\": \"A seated man cleans a shoe in a classroom setting with other individuals. the man\"}    \n",
    "    {\"prompt\": \"Two girls are sailing on a lake. they\"}\n",
    "    \n",
    "    Output: This function returns a list of prompts to be used by the draft LLM.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "\n",
    "test_json = json_loader(\"./hellaswag.json\") # Replace with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_embedding():\n",
    "    \"\"\"\n",
    "    This function rewrites your target LLM to include [PAD] token, which does not exist in the original LLaMA/LLaMA-2 tokenizer.\n",
    "    \"\"\"\n",
    "    # print(torch.cuda.device_count())\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
    "    dist.init_process_group(backend='nccl', rank=local_rank, world_size=world_size)\n",
    "    access_token = \"Your HF access token\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-hf\", padding_side=\"left\", token=access_token, torch_dtype=torch.float16, cache_dir=\"/pscratch/sd/m/myan/\")\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer.batch_encode_plus(examples, padding=\"longest\")['input_ids']\n",
    "\n",
    "    oracle_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-70b-hf\", torch_dtype=torch.float16)\n",
    "\n",
    "    oracle_model.resize_token_embeddings(len(tokenizer))\n",
    "    oracle_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    oracle_model.embed_tokens = nn.Embedding(oracle_model.config.vocab_size, oracle_model.config.hidden_size, padding_idx=oracle_model.config.pad_token_id)\n",
    "    \n",
    "    oracle_model.save_pretrained(\"/path/to/your/new/model\")\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your target LLM and desired tp degrees here.\n",
    "model_name = \"/path/to/your/new/model\"\n",
    "tensor_parallel_degrees = 4\n",
    "\n",
    "# Load the model on meta tensors from the config file.\n",
    "# This prevents deepspeed from loading models multiple times on each rank.\n",
    "config = AutoConfig.from_pretrained(model_name, cache_dir=\"/your/cache/dir/\")\n",
    "\n",
    "with deepspeed.OnDevice(dtype=torch.float16, device=\"meta\", enabled=True):\n",
    "    oracle_model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16)\n",
    "\n",
    "# Define the checkpoint dict. You may need to convert *.safetensors to\n",
    "# *.bin for this work. Make sure you get all the *.bin and *.pt files in\n",
    "# the checkpoint_files list.\n",
    "checkpoint_dir =\"/your/ckpt/dir/\"\n",
    "\n",
    "# Change ckpt names if your .bin files are named differently\n",
    "checkpoint_files = [\n",
    "    os.path.join(checkpoint_dir, f\"pytorch_model-{i:05d}-of-00029.bin\") \n",
    "    for i in range(1, 30) # Change number of bin files based on your model\n",
    "]\n",
    "\n",
    "checkpoint_dict = {\n",
    "    \"type\": \"DS_MODEL\",\n",
    "    \"checkpoints\": checkpoint_files,\n",
    "    \"version\": 1.0,\n",
    "}\n",
    "\n",
    "oracle_model = deepspeed.init_inference(\n",
    "                oracle_model,\n",
    "                replace_with_kernel_inject=False,\n",
    "                tp={\"tp_size\": tensor_parallel_degrees,},\n",
    "                dtype=torch.float16,\n",
    "                checkpoint=checkpoint_dict,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a08bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLaMA tokenizer does not have a pad token. \n",
    "# Modify the tokenizer to add a pad token and change the model configs accordingly.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-hf\", padding_side=\"left\", torch_dtype=torch.float16)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "# Feel free to change it to the draft model of your choice\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"minghaoyan/Wide-Sheared-LLaMA-796M\", torch_dtype=torch.float16)\n",
    "\n",
    "draft_model.resize_token_embeddings(len(tokenizer))\n",
    "draft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "draft_model.embed_tokens = nn.Embedding(draft_model.config.vocab_size, draft_model.config.hidden_size, padding_idx=draft_model.config.pad_token_id)\n",
    "\n",
    "\n",
    "# Launch the draft model with deepspeed on 1 node. Alternatively, you could use HF or load from a checkpoint.\n",
    "draft_model = deepspeed.init_inference(\n",
    "                draft_model,\n",
    "                replace_with_kernel_inject=False,\n",
    "                tp={\"tp_size\": 1,},\n",
    "                dtype=torch.float16,\n",
    "                #checkpoint=checkpoint_dict,\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b402cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_verification(model, input_tensor, max_new_tokens, local_rank, iter_count, past_key_values):\n",
    "    \"\"\"\n",
    "    Verifies the predictions of an oracle model by comparing the generated tokens against actual tokens.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The oracle model used for generating predictions.\n",
    "        input_tensor (torch.Tensor): The input tensor containing tokens for prediction.\n",
    "        max_new_tokens (int): The maximum number of new tokens to be generated by the model.\n",
    "        local_rank (int): The local rank identifier for distributed training.\n",
    "        iter_count (int): The current iteration count, used to determine the first call to the model.\n",
    "        past_key_values (torch.Tensor): Cached past key values for accelerating generation in subsequent calls.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - The positions of the first incorrect predictions in each row.\n",
    "               - The updated past_key_values tensor with dimensions adjusted based on the first incorrect prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If it's the first iteration, generate outputs using the full input tensor\n",
    "    if iter_count == 1:\n",
    "        outputs = model(input_tensor.unsqueeze(0).cuda(local_rank), use_cache=True)\n",
    "        new_past_key_values = outputs.past_key_values\n",
    "        \n",
    "        # Extract the token IDs predicted for the last max_new_tokens positions\n",
    "        next_token_id = outputs.logits[:, -max_new_tokens - 1:-1, :].argmax(dim=-1, keepdim=False)\n",
    "    else:\n",
    "        # For subsequent iterations, use past key values to accelerate generation\n",
    "        outputs = model(input_tensor.unsqueeze(0).cuda(local_rank), past_key_values=past_key_values, use_cache=True)\n",
    "        new_past_key_values = outputs.past_key_values\n",
    "        next_token_id = outputs.logits[:, :, :].argmax(dim=-1, keepdim=False)\n",
    "\n",
    "    # Extract the actual next tokens from the input tensor\n",
    "    actual_next_tokens_tensor = input_tensor[-max_new_tokens:]\n",
    "    \n",
    "    # Compare the predicted next tokens with the actual next tokens\n",
    "    correct_predictions = (next_token_id == actual_next_tokens_tensor)\n",
    "\n",
    "    # Convert the boolean tensor to a float tensor for further processing\n",
    "    correct_predictions_float = correct_predictions.float()\n",
    "\n",
    "    # Initialize a tensor to hold the positions of the first incorrect prediction in each row\n",
    "    first_false_positions = torch.full((correct_predictions_float.size(0),), correct_predictions_float.size(1), device=correct_predictions_float.device)\n",
    "    \n",
    "    # Check if there's any incorrect prediction within the max_new_tokens limit and adjust past_key_values accordingly\n",
    "    if first_false_positions < max_new_tokens:\n",
    "        return first_false_positions, new_past_key_values[:, :, :-max_new_tokens + first_false_positions, :]\n",
    "    \n",
    "    return first_false_positions, new_past_key_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prompts = []\n",
    "curr_count = 0\n",
    "\n",
    "# Set hyperparameters for speculative decoding\n",
    "batch_size = 1\n",
    "max_new_tokens = 7\n",
    "output_file = \"output-file.txt\"\n",
    "\n",
    "for batch in test_json:\n",
    "    \n",
    "    # Adding the prompt from the current batch to a list for processing\n",
    "    current_prompts.append(batch['prompt'])\n",
    "\n",
    "    # Keeping track of how many prompts have been processed\n",
    "    curr_count += 1\n",
    "    if curr_count == batch_size:\n",
    "        # If the current count reaches the batch size, encode the batch of prompts\n",
    "        draft_input_ids = tokenizer.batch_encode_plus(current_prompts, padding='longest')\n",
    "        # Reset the prompts list and count for the next batch\n",
    "        current_prompts = []\n",
    "        curr_count = 0\n",
    "        \n",
    "        # Handling ground truth data based on the batch size\n",
    "        if batch_size == 1:\n",
    "            # For single-item batches\n",
    "            ground_truth_slice = ground_truth_tensor_list[processed_batches]\n",
    "            ground_truth_tensor = ground_truth_slice.unsqueeze(0).cuda(local_rank)\n",
    "        else:\n",
    "            # For multi-item batches\n",
    "            ground_truth_slice = ground_truth_tensor_list[processed_batches*batch_size:(processed_batches+1)*batch_size]\n",
    "            ground_truth_tensor = torch.stack(ground_truth_slice, dim=0).cuda(local_rank)\n",
    "\n",
    "        # Calculating the maximum length for the generated sequence\n",
    "        max_length = ground_truth_tensor.size(1) - max_new_tokens - 2\n",
    "        current_length = 0\n",
    "        iter_count = 0\n",
    "\n",
    "        # Initializing a tensor to keep track of total matched tokens\n",
    "        total_matched = torch.zeros(batch_size, dtype=torch.int32).cuda(local_rank)\n",
    "\n",
    "        # Generating sequences up to the max length\n",
    "        while current_length < max_length:\n",
    "            if iter_count == 0:\n",
    "                # For the first iteration, use the input prompt\n",
    "                iter_count += 1\n",
    "                output_len = len(draft_input_ids[\"input_ids\"][0]) + max_new_tokens\n",
    "                input_tensors = torch.tensor(draft_input_ids[\"input_ids\"]).cuda(local_rank)\n",
    "            else:\n",
    "                # For subsequent iterations, use new inputs based on matched tokens\n",
    "                output_len = len(new_inputs[0]) + max_new_tokens\n",
    "                input_tensors = new_inputs\n",
    "                if batch_size == 1:\n",
    "                    input_tensors.unsqueeze(0)\n",
    "\n",
    "            # Generating predictions using the draft models if the local rank is within the models' range\n",
    "            if local_rank < len(draft_models):\n",
    "                draft_models[local_rank].cuda(local_rank)\n",
    "                padded_tensor = draft_models[local_rank].generate(input_tensors, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id).to(dtype=torch.int32)\n",
    "            else:\n",
    "                padded_tensor = torch.zeros(output_len, dtype=torch.int32).cuda(local_rank)\n",
    "\n",
    "            # Gathering predictions from all devices in a distributed setting\n",
    "            gathered_padded_tensors = [torch.zeros(output_len, dtype=torch.int32).cuda(local_rank) for _ in range(world_size)]\n",
    "            dist.all_gather(gathered_padded_tensors, padded_tensor.int())\n",
    "            cat_tensor = gathered_padded_tensors[0]\n",
    "\n",
    "            # Trimming the concatenated tensor if beyond the first iteration\n",
    "            if iter_count > 1:\n",
    "                cat_tensor = cat_tensor[-max_new_tokens:]\n",
    "            \n",
    "            # Verifying the generated sequence against the ground truth\n",
    "            matched_tokens, past_key_values = oracle_verification(oracle_model, cat_tensor, max_new_tokens, local_rank, iter_count, past_key_values)\n",
    "\n",
    "            # Updating the total matched tokens count\n",
    "            total_matched += matched_tokens + 1\n",
    "            \n",
    "            # Logging the total matched tokens to a file if the local rank is 0\n",
    "            if local_rank == 0:\n",
    "                with open(output_file, \"a\") as f:\n",
    "                    f.write(str(total_matched) + str(\"\\n\"))\n",
    "            \n",
    "            # Updating the current length for the next iteration\n",
    "            current_length = min(total_matched)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac93a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
